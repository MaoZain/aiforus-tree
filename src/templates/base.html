<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <title>Interactive Experience</title>
    <style>
        body { margin: 0; overflow: hidden; background-color: #000; }
        #canvas-container { width: 100vw; height: 100vh; position: absolute; top: 0; left: 0; z-index: 1; }
        #video-input { position: absolute; top: 0; left: 0; visibility: hidden; }
        #loading { position: absolute; top: 50%; left: 50%; transform: translate(-50%, -50%); color: white; font-family: sans-serif; z-index: 10; }
    </style>
    
    <!-- Import Maps for Three.js -->
    <script type="importmap">
        {
            "imports": {
                "three": "https://unpkg.com/three@0.160.0/build/three.module.js",
                "three/addons/": "https://unpkg.com/three@0.160.0/examples/jsm/",
                "@mediapipe/tasks-vision": "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.9/+esm"
            }
        }
    </script>

    <!-- Inject User Data -->
    <script>
        window.INITIAL_DATA = {{INITIAL_DATA_JSON}};
    </script>
</head>
<body>
    <div id="loading">Loading experience...</div>
    <video id="video-input" playsinline></video>
    <div id="canvas-container"></div>

    <script type="module">
        import * as THREE from 'three';
        import { FilesetResolver, GestureRecognizer } from '@mediapipe/tasks-vision';

        const data = window.INITIAL_DATA;
        console.log('Loaded Data:', data);

        // --- 1. Setup Three.js Scene ---
        const scene = new THREE.Scene();
        const camera = new THREE.PerspectiveCamera(75, window.innerWidth / window.innerHeight, 0.1, 1000);
        const renderer = new THREE.WebGLRenderer({ alpha: true, antialias: true });
        renderer.setSize(window.innerWidth, window.innerHeight);
        document.getElementById('canvas-container').appendChild(renderer.domElement);

        // Add lights
        const ambientLight = new THREE.AmbientLight(0xffffff, 0.5);
        scene.add(ambientLight);
        const directionalLight = new THREE.DirectionalLight(0xffffff, 1);
        directionalLight.position.set(0, 1, 1);
        scene.add(directionalLight);

        // Add a simple cube to prove Three.js is working
        const geometry = new THREE.BoxGeometry();
        const material = new THREE.MeshStandardMaterial({ color: data.theme === 'gold' ? 0xffd700 : 0x00ffff });
        const cube = new THREE.Mesh(geometry, material);
        scene.add(cube);
        camera.position.z = 5;

        // Handle Resize
        window.addEventListener('resize', () => {
            camera.aspect = window.innerWidth / window.innerHeight;
            camera.updateProjectionMatrix();
            renderer.setSize(window.innerWidth, window.innerHeight);
        });

        // --- 2. Setup MediaPipe (Placeholder) ---
        let gestureRecognizer;
        let runningMode = "VIDEO";
        let videoElement = document.getElementById("video-input");

        async function createGestureRecognizer() {
            const vision = await FilesetResolver.forVisionTasks(
                "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.9/wasm"
            );
            gestureRecognizer = await GestureRecognizer.createFromOptions(vision, {
                baseOptions: {
                    modelAssetPath: "https://storage.googleapis.com/mediapipe-models/gesture_recognizer/gesture_recognizer/float16/1/gesture_recognizer.task",
                    delegate: "GPU"
                },
                runningMode: runningMode
            });
            console.log("MediaPipe Loaded");
            document.getElementById('loading').style.display = 'none';
            enableCam();
        }

        // --- 3. Camera Access ---
        function enableCam() {
            if (!gestureRecognizer) return;

            const constraints = { video: true };
            navigator.mediaDevices.getUserMedia(constraints).then((stream) => {
                videoElement.srcObject = stream;
                videoElement.addEventListener("loadeddata", predictWebcam);
                videoElement.play();
            }).catch(err => {
                console.error("Camera access denied:", err);
                alert("Camera access is required for this experience.");
            });
        }

        let lastVideoTime = -1;
        async function predictWebcam() {
            let nowInMs = Date.now();
            if (videoElement.currentTime !== lastVideoTime) {
                lastVideoTime = videoElement.currentTime;
                const results = gestureRecognizer.recognizeForVideo(videoElement, nowInMs);
                
                // Interaction Logic Here based on results
                if (results.gestures.length > 0) {
                    const categoryName = results.gestures[0][0].categoryName;
                    console.log("Gesture:", categoryName);
                    // Example: Rotate cube on gesture
                    cube.rotation.x += 0.1;
                    cube.rotation.y += 0.1;
                }
            }
            
            // Render Three.js
            cube.rotation.x += 0.01;
            cube.rotation.y += 0.01;
            renderer.render(scene, camera);

            window.requestAnimationFrame(predictWebcam);
        }

        // Start
        createGestureRecognizer();

    </script>
</body>
</html>
